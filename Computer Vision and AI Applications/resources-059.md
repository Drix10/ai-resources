### ü§ñ Hedra Hiring - Referral Program

This article announces Hedra's new referral program for hiring passionate engineers across various specializations, fueled by recent Series A funding.  The company is experiencing rapid growth and seeks to expand its engineering team.

Key Points:

‚Ä¢ Hedra is scaling rapidly and hiring engineers.

‚Ä¢  A new referral program has been launched.

‚Ä¢  Open positions include frontend, fullstack, and backend engineers.


üîó Resources:

‚Ä¢ [Jon Barron](https://x.com/jon_barron) - Hedra team member

‚Ä¢ [M.J.L. Bach](https://x.com/mjlbach) - Hedra team member

![Image](https://pbs.twimg.com/media/GrpexrIWcAAC2W3?format=jpg&name=small)


---
### ü§ñ Large Language Model Instruction Following - Claude 4

This article discusses observations about Claude 4's instruction following capabilities, highlighting a surprising level of accuracy and instances where repeated instructions were necessary due to occasional prompt disregard.

Key Points:

‚Ä¢ Claude 4 demonstrates strong instruction following.

‚Ä¢  The model sometimes ignores parts of prompts.

‚Ä¢  Prompt engineering requires careful attention to detail.


üîó Resources:

‚Ä¢ [Kyem Agyei](https://x.com/KyemAgyei) - AI researcher

‚Ä¢ [Alex Albert](https://x.com/alexalbert__) - AI researcher

---
### ü§ñ Sarvam-M - Hybrid Model for Indian Languages

This article introduces Sarvam-M, a 24B parameter open-weight hybrid model built upon Mistral Small, showcasing improved performance in Indian languages, mathematics, and programming. A detailed technical blog post is referenced for further information.

Key Points:

‚Ä¢ Sarvam-M is a 24B parameter hybrid model.

‚Ä¢  It achieves new benchmarks across various tasks.

‚Ä¢  It's built on top of Mistral Small.


üîó Resources:

‚Ä¢ [Clement Delangue](https://x.com/ClementDelangue) -  AI researcher

‚Ä¢ [Sarvam AI](https://x.com/SarvamAI) - AI company

![Image](https://pbs.twimg.com/media/GroJe5jWwAAgY5A?format=jpg&name=small)


---
### ü§ñ ICRA 2025 - Egocentric Vision Talk

This article announces a presentation on egocentric vision for anticipation at the ICRA 2025 conference in Atlanta.  The author invites attendees to connect.

Key Points:

‚Ä¢  Presentation on egocentric vision at ICRA 2025.

‚Ä¢  Focus on anticipation in enhancing human mobility.


üîó Resources:

‚Ä¢ [Andrew Furnari](https://x.com/anfurnari) - Presenter

‚Ä¢ [ICRA 2025 Website](https://sites.google.com/andrew.cmu.edu/icra2025-vision-wearable-robot‚Ä¶) - Workshop information


![Image](https://pbs.twimg.com/media/Grom6kMXgAAV2Nh?format=jpg&name=small)
![Image](https://pbs.twimg.com/media/Grom6kJWIAAL0vD?format=jpg&name=360x360)
![Image](https://pbs.twimg.com/media/Grom6kXXMAA5auB?format=jpg&name=small)
![Image](https://pbs.twimg.com/media/Grom6kUWwAALL7B?format=jpg&name=small)


---
### ü§ñ SWE-Bench - Large Language Model Benchmark

This article discusses SWE-Bench, a benchmark verified by Ofir Press and OpenAI, focusing on its design and surprising results showing models struggling to achieve high accuracy despite the use of three annotators per issue.

Key Points:

‚Ä¢  SWE-Bench is a new benchmark for LLMs.

‚Ä¢ Models struggle to surpass 80-85% accuracy.

‚Ä¢ It's considered the first agentic benchmark.


üîó Resources:

‚Ä¢ [Giffmana](https://x.com/giffmana) - AI researcher

‚Ä¢ [Ofir Press](https://x.com/OfirPress) -  AI researcher

![Image](https://pbs.twimg.com/media/GroQk38X0AAr-jl?format=jpg&name=small)


---
### ü§ñ Sarvam-M - Hybrid Reasoning Model

This article introduces Sarvam-M, a 24B parameter hybrid reasoning model with strong performance in math, reasoning, coding, and Indic languages. The model is post-trained on Mistral-Small.

Key Points:

‚Ä¢ Sarvam-M excels in math, reasoning, and coding.

‚Ä¢  Strong performance in Indic languages.

‚Ä¢  Post-trained on Mistral-Small.


üîó Resources:

‚Ä¢ [auto_grad_](https://x.com/auto_grad_) - AI researcher

‚Ä¢ [selfawareatom](https://x.com/selfawareatom) - AI researcher

![Image](https://pbs.twimg.com/media/Grn_ic9WAAAFZnE?format=jpg&name=small)


---
### ü§ñ Anthropic Model Releases - Claude Opus 4 and Sonnet 4

This article announces the release of two major AI models from Anthropic: Claude Opus 4 (a top-performing coding model) and Sonnet 4 (a significant upgrade). The release also includes the Claude Code agentic tool.

Key Points:

‚Ä¢ Claude Opus 4 is a leading coding model.

‚Ä¢ Sonnet 4 represents a significant improvement.

‚Ä¢ Claude Code agentic tool is available for terminals and IDEs.


üîó Resources:

‚Ä¢ [Lakshmi](https://x.com/laks316) - AI researcher

‚Ä¢ [Dr. Cintas](https://x.com/dr_cintas) - AI researcher

![Image](https://pbs.twimg.com/media/GrkcscaWsAIiMe-?format=jpg&name=small)


---
### üöÄ LLM App Testing - DeepEval

This article introduces DeepEval, an open-source tool that simplifies LLM evaluation by creating a concise test suite. It helps identify optimal models, prompts, and architectures for AI workflows.

Key Points:

‚Ä¢  DeepEval simplifies LLM evaluation.

‚Ä¢ It uses a two-line test suite.

‚Ä¢ Compatible with various frameworks.


üîó Resources:

‚Ä¢ [Lakshmi](https://x.com/laks316) - AI researcher

‚Ä¢ [Akshay Pachaar](https://x.com/akshay_pachaar) -  AI researcher

![Image](https://pbs.twimg.com/media/GrjiA4SXIAApbAq?format=jpg&name=small)


---
### ü§ñ Falcon-H1-1.5b-deep - Efficient Large Language Model

This article discusses Falcon-H1-1.5b-deep, highlighting its efficiency and potential to significantly reduce computational costs compared to other models.  Its small size, low training cost, and Mamba2 architecture are key features.

Key Points:

‚Ä¢ Falcon-H1-1.5b-deep is a small and efficient model.

‚Ä¢ Requires significantly less compute than competitors.

‚Ä¢  Trained with only 3B tokens.


üîó Resources:

‚Ä¢ [Clement Delangue](https://x.com/ClementDelangue) - AI researcher

‚Ä¢ [TeortaxesTex](https://x.com/teortaxesTex) - AI researcher

![Image](https://pbs.twimg.com/media/Grln_SuWYAAzg1a?format=jpg&name=small)
![Image](https://pbs.twimg.com/media/GrloS0zWcAAzL5v?format=jpg&name=small)


---
### ü§ñ Hand Pose Dataset - Challenges and Solutions

This article discusses challenges in creating accurate 3D hand pose datasets from video, specifically highlighting limitations of Apple Vision Pro and the solution of using multiple RGB-D cameras.

Key Points:

‚Ä¢ Accurate 3D hand pose acquisition is difficult.

‚Ä¢ Apple Vision Pro lacks sufficient precision.

‚Ä¢ Multiple RGB-D cameras provide a better solution.


üîó Resources:

‚Ä¢ [Yu Xiang](https://x.com/YuXiang_IRVL) - Researcher

‚Ä¢ [HOCap Dataset](https://irvlutd.github.io/HOCap/) - Hand pose dataset

![Image](https://pbs.twimg.com/amplify_video_thumb/1925209068463181824/img/dARWDu3oq1Nuzsld.jpg)


---

### ‚≠êÔ∏è Support

If you liked reading this report, please star ‚≠êÔ∏è this repository and follow me on [Github](https://github.com/Drix10), [ùïè (previously known as Twitter)](https://x.com/DRIX_10_) to help others discover these resources and regular updates.

---