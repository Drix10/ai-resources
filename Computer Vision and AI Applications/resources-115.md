### ü§ñ 3D Graphics - Splatting to 3D Displays

This article discusses a novel algorithm that converts triangular and Gaussian splats into 3D display-compatible formats. This technology is being developed to support true defocus and parallax in 3D visualizations.

Key Points:

‚Ä¢ Converts splats to 3D display formats for enhanced visualization

‚Ä¢ Supports true defocus, providing realistic depth perception

‚Ä¢ Enables true parallax for accurate spatial representation

‚Ä¢ Leverages existing convex and Gaussian splatting techniques

üîó Resources:

‚Ä¢ [Brian C. Chao](https://x.com/BrianCChao) - Developer of the novel splatting conversion algorithm

‚Ä¢ [Tweet by Brian C. Chao](https://x.com/BrianCChao/status/1973919047587901881) - Original announcement thread

‚Ä¢ [Jan Held](https://x.com/janheld14) - Researcher in convex splatting

![Image](https://pbs.twimg.com/amplify_video_thumb/1973866703923916801/img/kasp1DakcDtWlYzQ.jpg)

---
### ü§ñ Computer Vision - Feed-Forward Camera Localization

This article discusses an approach to feed-forward camera localization from a collection of image features. It highlights an efficient method for 3D point-map estimation by using random subsampling of image data.

Key Points:

‚Ä¢ Utilizes a collection of image features for camera localization

‚Ä¢ Emphasizes random subsampling over full image pairs for efficiency

‚Ä¢ Enables faster computation in 3D point-map estimation

‚Ä¢ Allows processing a larger number of images for improved robustness

üöÄ Implementation:

1. Context 3D Point-Map Estimator: Provide relevant contextual data to the estimator for initialization.
2. Randomly Subsample Image Features: Select a random subset of image features instead of full image pairs.
3. Process Subsampled Features: Input the selected features into the estimator for accelerated computation.

üîó Resources:

‚Ä¢ [Kwang Moo Yi](https://x.com/kwangmoo_yi) - Discussing camera localization research

‚Ä¢ [Tweet by Kwang Moo Yi](https://x.com/kwangmoo_yi/status/1973832770087743989) - Original discussion on camera localization

![Image](https://pbs.twimg.com/amplify_video_thumb/1973832745018417153/img/EEURRjpMp-RE1sSU.jpg)

---
### ü§ñ Robotics - VLM2VLA for Multimodal Robot Policies

This article introduces VLM2VLA, a method for adapting Vision-Language Models (VLMs) into robot policies. It maintains multimodal generalization by treating robot actions as language and applying LoRA tuning.

Key Points:

‚Ä¢ Adapts Vision-Language Models (VLMs) for robot policy control

‚Ä¢ Treats robot actions as language for unified multimodal processing

‚Ä¢ Utilizes LoRA tuning to preserve core VLM generalization capabilities

‚Ä¢ Enables strong zero-shot generalization across new tasks and languages

üöÄ Implementation:

1. Represent Robot Actions as Language: Define robot actions using a linguistic format or lexicon.
2. Integrate with a VLM: Connect these language-represented actions to a Vision-Language Model.
3. Apply LoRA Tuning: Fine-tune the VLM using Low-Rank Adaptation for specific robot tasks.
4. Deploy for Zero-Shot Generalization: Test the adapted policy on novel tasks and languages.

üîó Resources:

‚Ä¢ [Visual AI Lab](https://x.com/VisualAILab) - Researching visual AI and robotics

‚Ä¢ [Cindy Wu](https://x.com/cindy_x_wu) - Discussing VLM2VLA development

‚Ä¢ [Tweet by Cindy Wu](https://x.com/cindy_x_wu/status/1973427949505589507) - Original announcement of VLM2VLA

![Image](https://pbs.twimg.com/amplify_video_thumb/1973406376433995776/img/AlgbsvkvqP3Ol7f5.jpg)

---
### ‚ú® AI Creativity - Sora for Video Generation

This article explores the capabilities of Sora in generating evocative video content from simple textual prompts. It reflects on how AI can create compelling visual narratives that resonate as true.

Key Points:

‚Ä¢ Utilizes Sora for generating video content based on textual prompts

‚Ä¢ Demonstrates AI's capacity to create "true-feeling" visual narratives

‚Ä¢ Offers a new paradigm for capturing and sharing moments without direct recording

‚Ä¢ Highlights the artistic potential of advanced AI video generation

üîó Resources:

‚Ä¢ [Create Compute](https://x.com/create_compute) - Sharing AI art and video creations

‚Ä¢ [Tweet by Create Compute](https://x.com/create_compute/status/1973780217664778449) - Original post on Sora video generation

![Image](https://pbs.twimg.com/amplify_video_thumb/1973766930612789248/img/mpqCLl4r26Zt4vsm.jpg)

![Image](https://pbs.twimg.com/media/G2Q6o-9XkAAkvFH?format=jpg&name=small)

![Image](https://pbs.twimg.com/media/G2Q6kPMXEAAVzMI?format=jpg&name=small)

---
### üöÄ Tools - Tinker API for LLM Fine-tuning

This article introduces Tinker, a flexible API designed for fine-tuning language models. It enables developers to define training loops locally, which are then efficiently executed on distributed GPU infrastructure.

Key Points:

‚Ä¢ Provides a flexible API for fine-tuning large language models

‚Ä¢ Allows local Python development for custom training loops

‚Ä¢ Manages execution on distributed GPU resources automatically

‚Ä¢ Supports researchers and developers using cutting-edge open models

üöÄ Implementation:

1. Develop Training Loops: Write Python training code for language models on a local machine.
2. Utilize Tinker API: Integrate the custom training logic with the Tinker API.
3. Submit for Distributed Execution: Leverage Tinker to run the training on distributed GPUs.
4. Monitor and Iterate: Observe results and refine models based on performance.

üîó Resources:

‚Ä¢ [Pranab Kumar](https://x.com/pranabkumar_) - Announcing Tinker API for LLM fine-tuning

‚Ä¢ [Thinky Machines](https://x.com/thinkymachines) - The team behind the Tinker API

‚Ä¢ [Tweet by Thinky Machines](https://x.com/thinkymachines/status/1973447428977336578) - Tinker API official launch

![Image](https://pbs.twimg.com/media/G2MX7iDaIAclPKq?format=png&name=small)

---
### ü§ñ AI Theory - LLMs, Continual Learning, and New Architectures

This article discusses Richard Sutton's perspective on Large Language Models (LLMs) and the fundamental requirement for continual learning architectures. It explores the idea that new architectural paradigms are crucial for effective on-the-job learning in AI.

Key Points:

‚Ä¢ Richard Sutton suggests current LLMs may not align with the "bitter lesson" of scaling

‚Ä¢ Advocates for new architectures enabling continual, on-the-job learning

‚Ä¢ Proposes that continual learning reduces the need for specialized pre-training

‚Ä¢ Challenges prevailing views on the future direction of LLM development

üîó Resources:

‚Ä¢ [Biruk Abere](https://x.com/Biruk_Abere) - Discussing AI and learning theory

‚Ä¢ [Dwarkesh Patel](https://x.com/dwarkesh_sp) - Curating discussions on AI advancements

‚Ä¢ [Tweet by Dwarkesh Patel](https://x.com/dwarkesh_sp/status/1971606180553183379) - Original discussion on Sutton's views

‚Ä¢ [Richard S. Sutton](https://x.com/RichardSSutton) - Father of reinforcement learning

---
### üöÄ Tools - Tinker API for Accessible LLM Training

This article announces Tinker, a new training API designed to democratize fine-tuning of large language models. It simplifies distributed training, allowing users to focus on data and algorithms without needing deep infrastructure expertise.

Key Points:

‚Ä¢ Tinker is presented as an accessible training API for all users

‚Ä¢ Abstracts away the complexities of distributed GPU training

‚Ä¢ Enables users to concentrate on their data and model algorithms

‚Ä¢ Facilitates training custom models without extensive infrastructure knowledge

üöÄ Implementation:

1. Define Model and Data: Prepare your language model and the training dataset.
2. Specify Algorithms: Outline the training algorithms and required parameters.
3. Integrate with Tinker API: Use the API to submit your custom training job.
4. Execute Distributed Training: Let Tinker handle the distributed GPU execution automatically.

üîó Resources:

‚Ä¢ [Pranab Kumar](https://x.com/pranabkumar_) - Co-founder of Thinky Machines

‚Ä¢ [Dhruv Chaplot](https://x.com/dchaplot) - Announcing Tinker, the new training API

‚Ä¢ [Thinky Machines](https://x.com/thinkymachines) - The company behind the Tinker API

‚Ä¢ [Tweet by Dhruv Chaplot](https://x.com/dchaplot/status/1973450051646267415) - Product launch announcement

‚Ä¢ [Tweet by Thinky Machines](https://x.com/thinkymachines/status/1973447428977336578) - Tinker API official launch details

![Image](https://pbs.twimg.com/media/G2MX7iDaIAclPKq?format=png&name=small)

---
### ü§ñ Robotics - Community Engagement in Robot Events

This article highlights Rerun.io's sponsorship of the REKrobot fight, emphasizing the importance of community engagement in robotics. The event successfully fostered enthusiasm and participation in robot experimentation and development.

Key Points:

‚Ä¢ Rerun.io supports community robotics events and initiatives

‚Ä¢ Encourages wider participation and experimentation with robots

‚Ä¢ Fosters a positive and entertaining environment for robotics enthusiasts

‚Ä¢ Demonstrates commitment to the growth of the robotics field

üîó Resources:

‚Ä¢ [Rerun.io](https://x.com/rerundotio) - Company supporting robotics community events

‚Ä¢ [REKrobot](https://x.com/REKrobot) - Robotics fight event organizer

‚Ä¢ [Tweet by Rerun.io](https://x.com/rerundotio/status/1973778272535126358) - Post on sponsoring the robot fight

![Image](https://pbs.twimg.com/ext_tw_video_thumb/1973778259566370816/pu/img/4BO-bwYG3Qhvuhh0.jpg)

---
### üí° Event - AutoSens for Automotive Advancements

This article highlights the AutoSens event, offering insights into the latest advancements in Advanced Driver-Assistance Systems (ADAS), Autonomous Vehicles (AV), and in-cabin monitoring. Attendees can engage with innovative demos and a comprehensive technical agenda.

Key Points:

‚Ä¢ Features innovative hands-on demonstrations of automotive technologies

‚Ä¢ Provides an insightful technical agenda on current automotive advancements

‚Ä¢ Covers the latest developments in ADAS technologies

‚Ä¢ Explores new solutions in Autonomous Vehicle systems

‚Ä¢ Discusses innovations in in-cabin monitoring solutions

üîó Resources:

‚Ä¢ [AutoSens](https://x.com/AutoSens_) - Organizer of the automotive technology event

‚Ä¢ [Event Pass](https://hubs.ly/Q03LT7Tn0) - Link to book passes for the AutoSens event

‚Ä¢ [Tweet by AutoSens](https://x.com/AutoSens_/status/1973696972230897837) - Announcement for the upcoming event

![Image](https://pbs.twimg.com/media/G2P67bbXUAAOWa0?format=jpg&name=360x360)

![Image](https://pbs.twimg.com/media/G2P67lfXoAA_kla?format=jpg&name=360x360)

![Image](https://pbs.twimg.com/media/G2P67ytWAAANkOO?format=jpg&name=360x360)

![Image](https://pbs.twimg.com/media/G2P67-oXsAAIOYV?format=jpg&name=360x360)

---
### ü§ñ Computer Vision - DA^2 for 360¬∞ Depth Estimation

This article introduces DA^2 (Depth Anything in Any Direction), a novel approach to 360¬∞ depth estimation. It details the creation of a large-scale training dataset and the subsequent training of a Sphere-aware Vision Transformer for comprehensive depth mapping.

Key Points:

‚Ä¢ Focuses on comprehensive 360¬∞ depth estimation capabilities

‚Ä¢ Involves creating a significantly scaled-up training dataset (10x larger)

‚Ä¢ Utilizes a Sphere-aware Vision Transformer (ViT) architecture

‚Ä¢ Aims to improve depth accuracy in all directions

üöÄ Implementation:

1. Collect or Scale Existing Data: Expand the training dataset for depth estimation by a factor of ten.
2. Preprocess Data for 360¬∞ Context: Prepare the data to be compatible with spherical perspectives.
3. Train a Sphere-aware ViT Model: Use the scaled and prepared data to train the Vision Transformer.
4. Evaluate 360¬∞ Depth Estimation: Assess the model's performance across all directions.

üîó Resources:

‚Ä¢ [Laks](https://x.com/laks316) - Discussing 360¬∞ depth estimation advancements

‚Ä¢ [Haodong Li](https://x.com/haodongli00) - Presenting the DA^2 project

‚Ä¢ [Akhaliq](https://x.com/_akhaliq) - Shared insights on the DA^2 project

‚Ä¢ [Tweet by Haodong Li](https://x.com/haodongli00/status/1973287870317338747) - Original announcement of DA^2

![Image](https://pbs.twimg.com/amplify_video_thumb/1973283608849948674/img/DVh5IUlXFzAuqqqK.jpg)


---

### ‚≠êÔ∏è Support

If you liked reading this report, please star ‚≠êÔ∏è this repository and follow me on [Github](https://github.com/Drix10), [ùïè (previously known as Twitter)](https://x.com/DRIX_10_) to help others discover these resources and regular updates.

---